# Deploy to Kops

```bash
export AWS_REGION=us-east-1
export SECURITY_GROUP_NAME=starter_api

aws ec2 create-security-group description ${SECURITY_GROUP_NAME} --group-name ${SECURITY_GROUP_NAME} --region ${AWS_REGION}
```

Output:

```json
{
"GroupId": "sg-01a7771c67b0a827a"
}
```

The GroupId variable will be used when creating the database instance, It’s then stored as an environmental variable for reuse. Utilizing the above GroupId as an example, the command is:

```bash
export SECURITY_GROUP_ID=sg-01a7771c67b0a827a
```

We need to add the appropriate permissions to the security group, this is done by running:

```bash
aws ec2 authorize-security-group-ingress \
--group-id ${SECURITY_GROUP_ID} \
--protocol tcp \
--port 5432 \
--cidr 0.0.0.0/0 \
--region ${AWS_REGION}
```

Output:

```json
{
    "Return": true,
    "SecurityGroupRules": [
        {
            "SecurityGroupRuleId": "sgr-07ee8aa4baf22eda0",
            "GroupId": "sg-01a7771c67b0a827a",
            "GroupOwnerId": "542991215884",
            "IsEgress": false,
            "IpProtocol": "tcp",
            "FromPort": 5432,
            "ToPort": 5432,
            "CidrIpv4": "0.0.0.0/0"
        }
    ]
}
```

Creating the database
In order to use an external Postgres database in the Kubernetes cluster, it needs to be created in AWS RDS. First we need to create environmental variables that can be reused:

```bash
export AWS_REGION=us-east-1
export RDS_DATABASE_NAME=starter-api-django
export RDS_TEMP_CREDENTIALS=starter_api_django
```

The database can then be created by running the following command:

```bash
aws rds create-db-instance \
--db-instance-identifier ${RDS_DATABASE_NAME} \
--db-name ${RDS_TEMP_CREDENTIALS} \
--vpc-security-group-ids ${SECURITY_GROUP_ID} \
--allocated-storage 20 \
--db-instance-class db.t3.medium \
--engine postgres \
--master-username ${RDS_TEMP_CREDENTIALS} \
--master-user-password ${RDS_TEMP_CREDENTIALS} \
--region ${AWS_REGION}
```

Output:

```json
{
    "EngineVersion": "13.3",
    "AutoMinorVersionUpgrade": true,
    "ReadReplicaDBInstanceIdentifiers": [],
    "LicenseModel": "postgresql-license",
    "OptionGroupMemberships": [
        {
                "OptionGroupName": "default:postgres-13",
                "Status": "in-sync"
        }
    ],
    "PubliclyAccessible": true,
    "StorageType": "gp2",
    "DbInstancePort": 0,
    "StorageEncrypted": false,
    "DbiResourceId": "db-6SDZPCLMKXURDES34FVBEVS2XI",
    "CACertificateIdentifier": "rds-ca-2019",
    "DomainMemberships": [],
    "CopyTagsToSnapshot": false,
    "MonitoringInterval": 0,
    "DBInstanceArn": "arn:aws:rds:us-east-1:542991215884:db:starter-api-django",
    "IAMDatabaseAuthenticationEnabled": false,
    "PerformanceInsightsEnabled": false,
    "DeletionProtection": false,
    "AssociatedRoles": [],
    "TagList": [],
    "CustomerOwnedIpEnabled": false
}
```

This might take a few minutes for the database creation to complete.
Do not forget to delete the database when you are done with the tutorial otherwise significant cost will be incurred.
Once created, the command to get the RDS endpoint is:

```bash
aws rds describe-db-instances \
--db-instance-identifier ${RDS_DATABASE_NAME} \
--region ${AWS_REGION}
```

Keep trying to run the above command if you don’t see a similar result to the following embedded in the output:

```
{...
"Endpoint": {
  "Address": "kubernetes-django-rds-sample.<hash>.us-east-1.rds.amazonaws.com",
  "Port": 5432,
  "HostedZoneId": "Z2R2ITUGPM61AM"
}
...}
```

Using Amazon RDS Postgres will require some changes to the existing manifest files which include:
Not utilizing the PersistentVolume subsystem as well as not having a Postgres deployment controller.
Create a service definition to use the AWS RDS endpoint, which means any configuration that is dependent on the postgres database will remain the same. Such behaviour is desirable in loosely coupled systems.
Create a Secret manifest file using the new credentials
We will be using the Endpoint value from the describe-db-instances command we ran earlier. The configuration file for the RDS service becomes

```yaml
kind: Service
apiVersion: v1
metadata:
  name: postgres-service
spec:
  type: ExternalName
  externalName: starter-api-django.ct8tg4hpu9vr.us-east-1.rds.amazonaws.com
```

Next we need to have a mechanism to pass the username and password into the Kubernetes cluster, this can be done by creating the ./rds/secrets.yaml file with the following configuration:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres-credentials
type: Opaque
data:
  user: a3ViZXJuZXRlc19kamFuZ28=
  password: a3ViZXJuZXRlc19kamFuZ28=
```

The username and password is a base64 encoded string generated by executing:

```bash
echo -n ${RDS_TEMP_CREDENTIALS} | base64
```

This is not a secure setup as the file can be stored in source control. A more secure option might be to create the secret resource imperatively using the kubectl create secret shell command.

Create a LoadBalancer Service in api

install kops ```brew update && brew install kops```

```bash
export BUCKET_NAME=k8-starter-api
aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION}
export KOPS_STATE_STORE=s3://${BUCKET_NAME}
```

```json
{
    "Location": "/k8-starter-api"
}
```

### Route53 Domain

Kops is an opinionated provisioning system which uses a valid DNS name as the cluster name, as well as for discovery inside the cluster in order to reach the Kubernetes API service. There are several advantages for doing this which include; unambiguously sharing the clusters, and not having to remember the IP address. It is advised the DNS name should be a subdomain (for example the one I used when testing was starter.api.tech4dev.nl). Creating the Route53 DNS is beyond the scope of this tutorial but several documentation exists on how to do this. The DNS name is stored as an environmental variable for reuse:

### Create Hosted Zone for Cluster

For this we require a  hosted zone associated with Route 53 which must be publicly resolvable. (However KOPS also allows to use private DNS which is more tricky.)

```bash
aws route53 create-hosted-zone --name danielale.com --caller-reference 2017-02-24-11:12 --hosted-zone-config Comment="Hosted Zone for KOPS"
```

Output:

```json
{
    "Location": "https://route53.amazonaws.com/2013-04-01/hostedzone/Z087974022JY5W7RCVDGV",
    "HostedZone": {
        "Id": "/hostedzone/Z087974022JY5W7RCVDGV",
        "Name": "danielale.com.",
        "CallerReference": "2017-02-24-11:12",
        "Config": {
            "Comment": "Hosted Zone for KOPS",
            "PrivateZone": false
        },
        "ResourceRecordSetCount": 2
    },
    "ChangeInfo": {
        "Id": "/change/C09958492H28896MGC9UI",
        "Status": "PENDING",
        "SubmittedAt": "2021-10-27T12:19:59.628000+00:00"
    },
    "DelegationSet": {
        "NameServers": [
            "ns-664.awsdns-19.net",
            "ns-81.awsdns-10.com",
            "ns-1901.awsdns-45.co.uk",
            "ns-1201.awsdns-22.org"
        ]
    }
}
```

```bash
export NAME=cluster.danielale.com
```

## Create Cluster

The cluster can then be created as follows (can take up to 15 minutes):

```bash
kops create cluster \
--name ${NAME} \
--ssh-public-key=~/cluster/devops23.pub \
--zones us-east-1a,us-east-1b,us-east-1c \
--state ${KOPS_STATE_STORE} \
--node-size t2.medium \
--master-size m3.medium \
--node-count 5 \
--dns-zone danielale.com \
--topology private \
--networking calico \
--bastion \
--yes
```

```bash
kubectl config current-context
```

Output: ```cluster.danielale.com```

Where the output should be the DNS name that was given to the cluster. In order to confirm that the cluster was created, the following command can be used (it can take a few minutes to complete):

```kubectl get nodes```